{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc035e8-5d3e-4771-b1b6-dc157d4aafb2",
   "metadata": {},
   "source": [
    "Link demo: https://share.streamlit.io/thanhtruong1712/fake-news-detection/streamlit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b833d-1eb0-4771-b3dc-601690a7bc33",
   "metadata": {},
   "source": [
    "Tiến độ hoàn thành của Project : 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd29dc-ed8c-4cf6-9323-d0d73d0af70a",
   "metadata": {},
   "source": [
    "## Danh sách thành viên trong nhóm\n",
    "| MSSV        | Họ và tên          | \n",
    "| ----------- | -----------------  | \n",
    "| 1712770     | Trương Thị Lệ Thanh|\n",
    "| 1712771     | Bùi Thái Tấn Thành | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc26d25-35c2-4ad3-9ae9-cdd681e25e1a",
   "metadata": {},
   "source": [
    "## Bảng phân công công việc\n",
    "| Công việc | Thành viên phụ trách | Mức độ hoàn thành | \n",
    "| --------| -----------------  | ---------------------|\n",
    "| Tiền xử lý văn bản tiếng Việt | Thanh & Thành | 100% |\n",
    "| EDA (khám phá dữ liệu) | Thanh & Thành | 100% |\n",
    "| Mô hình hóa | Thanh & Thành | 100% |\n",
    "| Deploy mô hình | Thanh & Thành | 100% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70b860-b67f-4a28-a253-2b9547cc8ab9",
   "metadata": {},
   "source": [
    "# I. Dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55321420-ac4d-4b1c-9153-a41d3ca64d17",
   "metadata": {},
   "source": [
    "## Thông tin dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5ae99-6118-4eb2-a8a6-4d5373111e04",
   "metadata": {},
   "source": [
    "#### Nguồn dữ liệu: VNFD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfc5d6-8ebb-435a-bf03-2e632165fb15",
   "metadata": {},
   "source": [
    "- Tập dữ liệu 223 record bản tin tiếng Việt, gồm 2 nhãn: 1 (tin giả) và 0 (tin thật)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a5744-71f7-4d27-8491-e5f04b51a42d",
   "metadata": {},
   "source": [
    "# II. Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dbc9cb1-61de-4d98-b6ec-e18f0bba3373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 01:24:47.472123: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-08 01:24:47.472208: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/thanhtruong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/thanhtruong/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/thanhtruong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import sklearn\n",
    "import pickle\n",
    "import tensorflow\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import plot_confusion_matrix,roc_curve,auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab6757-0c63-4ef8-ad4a-68f62445b9ae",
   "metadata": {},
   "source": [
    "# III. Khám phá dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c182fb-c9ac-4a3d-b2dc-f51161e3b730",
   "metadata": {},
   "source": [
    "### Đọc dữ liệu từ file csv vào dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdad32b-8b85-451a-ba1c-934fb79bb522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...</td>\n",
       "      <td>binhluan.biz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...</td>\n",
       "      <td>www.ipick.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...</td>\n",
       "      <td>tintucqpvn.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chưa bao giờ nhạc Kpop lại dễ hát đến thế!!!\\n...</td>\n",
       "      <td>tintucqpvn.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đại học Hutech sẽ áp dụng cải cách \"Tiếq Việt\"...</td>\n",
       "      <td>www.gioitreviet.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text               domain  \\\n",
       "0  Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...         binhluan.biz   \n",
       "1  Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...         www.ipick.vn   \n",
       "2  Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...       tintucqpvn.net   \n",
       "3  Chưa bao giờ nhạc Kpop lại dễ hát đến thế!!!\\n...       tintucqpvn.net   \n",
       "4  Đại học Hutech sẽ áp dụng cải cách \"Tiếq Việt\"...  www.gioitreviet.net   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('vn_news_223_tdlfr.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f058e50-2a5a-4d84-8970-d49041a56de2",
   "metadata": {},
   "source": [
    "### Dữ liệu gồm có bao nhiêu dòng và bao nhiêu cột?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b88a31-fe95-465d-b39b-a643c0067d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu có 223 dòng và 3 cột\n"
     ]
    }
   ],
   "source": [
    "rows, collumns = df.shape\n",
    "print(\"Dữ liệu có {} dòng và {} cột\".format(rows, collumns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb481562-2011-497b-8d16-a76b6eb24404",
   "metadata": {},
   "source": [
    "## 1.  Tiền xử lý văn bản tiếng Việt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbd37e-eabf-4bc7-8e4a-dd331ef49182",
   "metadata": {},
   "source": [
    "### Các bước tiền xử lý văn bản cơ bản gồm: \n",
    "- Lowercase\n",
    "- Loại stopwords\n",
    "- Tokenizer\n",
    "- Stemming\n",
    "- Loại noise\n",
    "- Loại dấu câu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9859d2-8238-4744-a8ad-b5325d900c06",
   "metadata": {},
   "source": [
    "Nguồn tham khảo stopword: https://www.kaggle.com/mpwolke/vietnamese-stopwords-w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79217cdd-a597-4d6f-b185-e61558b60370",
   "metadata": {},
   "source": [
    "#### Đọc dữ liệu stopwords từ file txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9aa44-7e95-4303-82ef-c3348a0f8058",
   "metadata": {},
   "source": [
    "Link file txt của stopword: https://www.kaggle.com/mpwolke/vietnamese-stopwords-w2v/data?select=vietnamese.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea365870-5557-40d0-a398-df2d5525a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list(filename):\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        stopwords=f.readlines()\n",
    "        stopset=set(m.strip() for m in stopwords)\n",
    "        return list(frozenset(stopset))\n",
    "stopwords=set(get_stopword_list('vietnamese.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b7a8a-79e8-4dfd-bd42-b8e322426f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b992e-4c8a-40ad-aa97-4c31a92ee7b8",
   "metadata": {},
   "source": [
    "#### Hàm tiền xử lý văn bản tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5d2f1-50c3-4679-a11d-5c47503987e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_basic(text):\n",
    "    text = re.sub(r'http\\S+','', text)  #Loại noise(xóa link)\n",
    "    text = re.sub(\"\\\\W\", ' ', text)     # Xóa khoảng trắng thừa\n",
    "    #Loại tokenizer và dấu câu\n",
    "    token_doc = word_tokenize(text)\n",
    "    result_token = []\n",
    "    for i in token_doc:\n",
    "        new_token = regex.sub(u'',i)\n",
    "        if not new_token == u'':\n",
    "            result_token.append(new_token)\n",
    "    #Loại stopwords\n",
    "    result_stopwords = []\n",
    "    for text in result_token:\n",
    "        tmp = text.split(' ')\n",
    "        for i in tmp:\n",
    "            if not i in stopwords :\n",
    "                result_stopwords.append(i)\n",
    "    #Xử lý stemming và lemmatizion\n",
    "    final_doc = []\n",
    "    for i in result_stopwords:\n",
    "        final_doc.append(wordnet.lemmatize(i))\n",
    "    return ' '.join(final_doc).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08586b32-a649-4ea5-a73f-3ac119875bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df.text = df.text.progress_map(preprocessing_basic)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc62a17",
   "metadata": {},
   "source": [
    "## 2. EDA (Khám phá dữ liệu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c164c",
   "metadata": {},
   "source": [
    "#### Thông tin dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d17f70",
   "metadata": {},
   "source": [
    "Ta thấy dữ liệu không bị thiếu, kiểu dữ liệu không sai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675e879-1cad-45dc-a7a1-b03e61858a8f",
   "metadata": {},
   "source": [
    "#### Kiểm tra các dòng có bị lặp không?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Số dòng dữ liệu bị lặp: ',df.index.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18fe166-31cf-42a1-a9be-e0b962564343",
   "metadata": {},
   "source": [
    "#### Kiểm tra phân bố các class có chênh lệch không?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587c24f-f514-4cf7-a20c-f20c74ce7fe2",
   "metadata": {},
   "source": [
    "#### Các thông tin thống kê của văn bản "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2350791-636d-42b7-b2f9-096c0f5d3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_len = df.text.str.len()\n",
    "print(\"Chiều dài trung bình của mỗi record: \",(record_len.sum()/len(record_len)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbd133-c17a-44a2-b9cc-d66fd7b9f570",
   "metadata": {},
   "source": [
    "# IV. Mô hình hóa dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hàm vẽ ma trận\n",
    "def plot_matrix(classifier,X_test,Y_test):\n",
    "    class_name=df.label.value_counts()\n",
    "    np.set_printoptions(precision=2)\n",
    "    titles_options=[(\"confusion matrix,without normalization\",None),(\"normalized confusion matrix\",None)]\n",
    "    for title,normalize in titles_options:\n",
    "        disp=plot_confusion_matrix(classifier,X_test,Y_test,display_labels=class_name,cmap=plt.cm.Blues,normalize=normalize)\n",
    "        disp.ax_.set_title(title)\n",
    "        print(title)\n",
    "        print(disp.confusion_matrix)\n",
    "\n",
    "        \n",
    "def upload_classifier(classifier):\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(df.text,df.label,test_size=0.25,random_state=2020)\n",
    "    pipe=Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('model',classifier)])\n",
    "    model=pipe.fit(X_train,Y_train)\n",
    "    prediction=model.predict(X_test)\n",
    "    prob=model.predict_proba(X_test)\n",
    "    plot_matrix(classifier=model,X_test=X_test,Y_test=Y_test)\n",
    "    return model,prob,Y_test\n",
    "\n",
    "def generate_roc_curve(model,Y_test,prob,title):\n",
    "    pred=prob[:,1]\n",
    "    fpr,tpr,threshold=roc_curve(Y_test,pred)\n",
    "    roc_auc=auc(fpr,tpr)\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(fpr,tpr,'b',label='auc=%0.2f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5aa8d-40ca-4f13-84fc-e7d7f62ffd66",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38798a9f-c03b-46ea-bdbb-10e3f2173c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model,prob,Y_test=upload_classifier(classifier=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a1de5-57c1-4aa9-aaf3-ccdd6d024d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_roc_curve(model=LR_model,Y_test=Y_test,prob=prob,title='Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b056f62-e66e-4159-a181-64279bb82a1c",
   "metadata": {},
   "source": [
    "## Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01b908-542b-4a0f-a07f-a26b944cd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC_model,prob,Y_test=upload_classifier(classifier=GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c2a02e-13ae-45d2-b806-d46bfef91b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_roc_curve(model=GBC_model,Y_test=Y_test,prob=prob,title='Gradient Boosting Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a152a6-c52f-4c5a-a191-5c1ac8067b14",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b692f1-8489-429b-83f5-e78660c2de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB_model,prob,Y_test=upload_classifier(classifier=MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea84d14-503a-4617-b819-26ed3e834f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_roc_curve(model=MNB_model,Y_test=Y_test,prob=prob,title='Multinomial Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b7c03-58fc-4995-ac58-b966286d9a41",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4268808-b133-48f8-a2ca-1df8e62d2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNB_model,prob,Y_test=upload_classifier(classifier=BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531e537-d031-4f86-ad20-9ed9436e2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_roc_curve(model=BNB_model,Y_test=Y_test,prob=prob,title='Bernoulli Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a01be-2298-490b-a784-9a50106f2dda",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699e314-fc78-4431-878a-a2db5d607490",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTs_model,prob,Y_test=upload_classifier(classifier=DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bf8de-22ea-4521-b605-d79692ff4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_roc_curve(model=DTs_model,Y_test=Y_test,prob=prob,title='Decision Trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a6a6d-3c8d-4996-ad54-919932cc4433",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd179e5-f36d-439c-9961-8ebf483e0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_model,prob,Y_test=upload_classifier(classifier=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b70c8-87b5-423c-a57d-7f6d2b035742",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_roc_curve(model=RFC_model,Y_test=Y_test,prob=prob,title='Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552445bb-3fe1-429e-91f2-56381c9213a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_list = [r\"models/LR_model.pkl\", r\"models/GBC_model.pkl\", r\"models/MNB_model.pkl\",\n",
    "                   r\"models/BNB_model.pkl\", r\"models/DTs_model.pkl\", r\"models/RFC_model.pkl\"]\n",
    "\n",
    "model_list = [LR_model, GBC_model, MNB_model, BNB_model, DTs_model, RFC_model]\n",
    "\n",
    "for model, filename in zip(model_list, model_file_list):\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75e352-b6df-40b6-955f-cd1bc5533ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenizer(text, maxword):\n",
    "    tokenizer = Tokenizer(num_words = maxword)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    with open('tokenizer.pickle','wb') as f:\n",
    "        pickle.dump(tokenizer, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "pre_tokenizer(df.text, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be3cf7-178b-4847-b86d-bb3b37e9cc7b",
   "metadata": {},
   "source": [
    "## Long-short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b54cc-8533-42ff-9dad-658531d9a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_testdata(text, tokenizer, Y):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(\"Vocabulary size: \", len(word_index))\n",
    "    data = pad_sequences(sequences, padding = 'post', maxlen = 200)\n",
    "    print(\"Shape of data tensorflow: \", data.shape)\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = Y[indices]\n",
    "    num_validation_samples = int(0.2*data.shape[0])\n",
    "    x_train = data[: -num_validation_samples]\n",
    "    y_train = labels[: -num_validation_samples]\n",
    "    x_val = data[-num_validation_samples: ]\n",
    "    y_val = labels[-num_validation_samples: ]\n",
    "    print(\"Number of training: \", y_train.sum(axis = 0))\n",
    "    print(\"Number of validation: \", y_val.sum(axis = 0))\n",
    "    print(\"Number of tokenized sentences:\\n\", data[0])\n",
    "    print(\"Number of one-hot label:\\n \", labels[0])\n",
    "    return x_train, y_train, x_val, y_val\n",
    "    \n",
    "def prepare_feature(df, label, text):\n",
    "    y = df[label].values\n",
    "    comment_train = df[text]\n",
    "    comment_train = list(comment_train)\n",
    "    return comment_train, y\n",
    "\n",
    "def get_dummies(df, col_label):\n",
    "    df_y = pd.get_dummies(df[col_label])\n",
    "    df_new = df.join(df_y)\n",
    "    df_new = df_new.drop(col_label, axis = 1)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7edbbc7-d709-4221-8045-68d1c4ebe02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.get_dummies(df.label)\n",
    "df_label = df_label.reset_index()\n",
    "df_label = df_label.drop('index', axis = 1)\n",
    "df_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc361d-129a-42a3-8ba0-c68f21594c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.join(df_label)\n",
    "df_new = get_dummies(df = df, col_label = 'label')\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871b1fc-963b-4d34-bc77-4374f41f8561",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1]\n",
    "x_train, y_train = prepare_feature(df = df_new, label = labels, text = 'text')\n",
    "print(x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c28d6-6c49-40df-8dd2-76fd9bf89c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle','rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312b275-6da0-4817-881a-738ab4371f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = prepare_testdata(text = x_train, tokenizer = tokenizer, Y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea53a85-8ddc-487d-a3d1-e49b0d701985",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.random.random((len(word_index)+1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1db9eb-6fdb-477c-9f7b-f1b587ea0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape = (200,), dtype = 'int32')\n",
    "embedding_layer = Embedding(len(word_index)+1, 100, weights = [embedding_matrix], input_length = 200, trainable = False, name = 'embeddings')\n",
    "embedded_sequences = embedding_layer(sequence_input);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b5e01-b87c-405d-b442-51f6b6ac8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LSTM(60, return_sequences = True, name = 'LSTM_layer')(embedded_sequences)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation = 'relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "pred = Dense(2, activation = 'sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e617e7-f52f-41b5-bb39-ba53e51d84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(sequence_input, pred)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4440c7-7aa2-4d0d-8b74-66b4056a3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 20, batch_size = 32, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d53d6-8d40-435b-8c47-0d75cdb8d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r\"models/LSTM_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fc462-37c0-49a0-918a-0ce7826361a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, label = 'training_loss')\n",
    "plt.plot(epochs, val_loss, label = 'validation_loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
